import tensorflow as tf
import tensorflow.contrib.layers as layer
import os
import numpy as np
from PIL import Image

os.environ['CUDA_VISIBLE_DEVICES'] = '1'

# image_size = 150*150
# image_channel = 1

MODEL_SAVE_PATH = "/home/yangguang/MODEL/"

MODEL_NAME = 'Siamese_vgg_for_gray_model.ckpt'


# INPUT NPY
batch_x0 = np.load('batch_x0.npy')
batch_y0 = np.load('batch_y0.npy')
batch_x1 = np.load('batch_x1.npy')
batch_y1 = np.load('batch_y1.npy')

x0 = tf.placeholder(tf.float32, [None, 150, 150, 1], name='input0')
x1 = tf.placeholder(tf.float32, [None, 150, 150, 1], name='input1')
y_ = tf.placeholder(tf.float32, [None, 1], name='label')



def siameseVGG(input):
    conv1_1 = tf.layers.conv2d(inputs=input, filters=32, kernel_size=3, strides=1,padding='same',
                               activation=tf.nn.relu, name='conv1_1')
    print(conv1_1.shape)

    conv1_2 = tf.layers.conv2d(conv1_1, filters=32, kernel_size=3, strides=1,padding='same',
                               activation=tf.nn.relu, name='conv1_2')
    print(conv1_2.shape)

    pool1 = tf.layers.max_pooling2d(conv1_2, pool_size=2, strides=2, name='pool1')
    print(pool1.shape)

    conv2_1 = tf.layers.conv2d(pool1, filters=64, kernel_size=3, strides=2,padding='same',
                               activation=tf.nn.relu, name='conv2_1')
    print(conv2_1.shape)

    conv2_2 = tf.layers.conv2d(conv2_1, filters=64, kernel_size=3, strides=1,padding='same',
                               activation=tf.nn.relu, name='conv2_2')
    print(conv2_2.shape)

    pool2 = tf.layers.max_pooling2d(conv2_2,pool_size=2, strides=1,name='pool2',padding='same')
    print(pool2.shape)

    conv3_1 = tf.layers.conv2d(pool2,filters=64, kernel_size=3, strides=1,padding='same',
                               activation=tf.nn.relu, name='conv3_1')
    print(conv3_1.shape)
    conv3_2 = tf.layers.conv2d(conv3_1, filters=64, kernel_size=3, strides=1, padding='same',
                               activation=tf.nn.relu, name='conv3_2')
    print(conv3_2.shape)
    conv3_3 = tf.layers.conv2d(conv3_2, filters=64, kernel_size=3, strides=1, padding='same',
                               activation=tf.nn.relu, name='conv3_3')
    print(conv3_3.shape)
    pool3 = tf.layers.max_pooling2d(conv3_3,pool_size=2,strides=1,name='pool3',padding='same')
    print(pool3.shape)

    conv4_1 = tf.layers.conv2d(pool3,filters=128,kernel_size=3,strides=1,padding='same',
                               activation=tf.nn.relu,name='conv4_1')
    print(conv4_1.shape)

    conv4_2 = tf.layers.conv2d(conv4_1,filters=128,kernel_size=3,strides=1,padding='same',
                               activation=tf.nn.relu,name='conv4_2')
    print(conv4_2.shape)

    conv4_3 = tf.layers.conv2d(conv4_2,filters=128,kernel_size=3,strides=1,padding='same',
                               activation=tf.nn.relu,name='conv4_3')
    print(conv4_3.shape)

    pool4 = tf.layers.max_pooling2d(conv4_3, pool_size=2, strides=2,name='pool4',padding='same')

    print(pool4.shape)

    conv5_1 = tf.layers.conv2d(pool4,filters=256,kernel_size=3,strides=1,padding='same',
                               activation=tf.nn.relu,name='conv5_1')
    print(conv5_1.shape)

    conv5_2 = tf.layers.conv2d(conv5_1,filters=256,kernel_size=3,strides=1,padding='same',
                               activation=tf.nn.relu,name='conv5_2')
    print(conv5_2.shape)

    conv5_3 = tf.layers.conv2d(conv5_2,filters=256,kernel_size=3,strides=1,padding='same',
                               activation=tf.nn.relu,name='conv5_3')
    print(conv5_3.shape)

    pool5 = tf.layers.max_pooling2d(conv5_3,pool_size=2,strides=1,name='pool5',padding='same')

    print(pool5.shape)

    fc0 = tf.layers.dense(layer.flatten(pool5), 512, name='fc0')


    print(fc0.shape)
    fc1 = tf.layers.dense(layer.flatten(fc0), 256, name='fc1')


    fc2 = tf.layers.dense(layer.flatten(fc1), 128, name='fc2')

    print(fc1.shape)

    feature_out = tf.multiply(fc2, tf.constant(value=1, dtype=tf.float32), name='feature_out')
    print(feature_out)

    return fc2


with tf.variable_scope('siamese') as scope:
    o0 = siameseVGG(x0)
    scope.reuse_variables()
    o1 = siameseVGG(x1)

with tf.variable_scope('ContrastiveLoss'):
    margin = 1.0
    labels_t = y_   # common pairs labels
    labels_f = tf.subtract(1.0, y_, name="1-yi")  # different pairs labels

    eucd2 = tf.pow(tf.subtract(o0, o1), 2)
    eucd2 = tf.reduce_sum(eucd2, 1)
    eucd = tf.sqrt(eucd2 + 1e-6, name="eucd")
    C = tf.constant(margin, name="C")   # define constant value C
    pos = tf.multiply(labels_t, eucd2, name="yi_x_eucd2")
    neg = tf.multiply(labels_f, tf.pow(tf.maximum(tf.subtract(C, eucd), 0), 2), name="Nyi_x_C-eucd_xx_2")
    losses = tf.add(pos, neg)
    loss = tf.reduce_mean(losses, name="loss")
    tf.summary.scalar('loss', loss)

train_step = tf.train.AdamOptimizer(learning_rate=1e-6).minimize(loss)
saver = tf.train.Saver()

init = tf.global_variables_initializer()

merge = tf.summary.merge_all()

iteration = 100
batch_size = 60
batch_size_test = 60


aabb = 0

with tf.Session() as sess:
    sess.run(init)
    for c in tf.trainable_variables():
        print(c)
    train_writer = tf.summary.FileWriter(logdir="logsV32/train",graph=sess.graph)
    test_writer = tf.summary.FileWriter(logdir="logsV32/test")
    saver.save(sess,'/home/yangguang/MODEL/Siamese_vgg_for_gray_model.ckpt')

    # batches_count = int(input_count / batch_size)
    # remainder = input_count % batch_size
    # print("Dataset is dividing to %s batches, each batch has %s dataï¼Œand the final batch has %s data" % (batches_count + 1, batch_size, remainder))

    for step in range(iteration):
        # The key is to translate input data into np.array
        batch_x00 = (batch_x0[step * batch_size:(step + 1) * batch_size]).reshape(-1, 150, 150, 1)
        batch_y00 = batch_y0[step * batch_size:(step + 1) * batch_size]
        batch_x11 = (batch_x1[step * batch_size:(step + 1) * batch_size]).reshape(-1, 150, 150, 1)
        batch_y11 = batch_y1[step * batch_size:(step + 1) * batch_size]
        batch_y = np.zeros((batch_size,1))
        for i in range(len(batch_y11)):
            if batch_y00[i] == batch_y11[i]:
                batch_y[i] = 1
            else:
                batch_y[i] = 0
        _, summary,train_loss = sess.run([train_step,merge,loss],
            feed_dict={x0: batch_x00,
                       x1: batch_x11,
                       y_: batch_y
                       })
        print('step %d: loss %.3f' % (step, train_loss))
train_writer.close()
test_writer.close()
